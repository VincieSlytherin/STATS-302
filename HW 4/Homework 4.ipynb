{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Classification and Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Ran Ju*\n",
    "Netid: rj133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Conceptual questions\n",
    "### (1.5 points total)\n",
    "\n",
    "**(a)** Considering a binary problem: Is it possible to compute an ROC curve for a 5-Nearest Neighbor classifier? (0.5 points)  Is it possible to compute an ROC curve for a decision tree with no limitation on the maximal depth?  (0.5 points) Explain your answers.\n",
    "\n",
    "**(b)** You produce ten bootstrapped samples from a data set containing classes A and B. Then you use a pre-trained logistic regression classifier on each sample and produce ten estimates of P(class = A): {0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75}. What are the predictions of a soft voting and a hard voting classifier? (0.5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) It is **possible** to compute an ROC curve for a 5-Nearest Neighbor classifier because becausthee  output of the can be probability thus we can compute the curve.<br>\n",
    "&emsp;&ensp;It is **impossible** to compute an ROC curve for a decision tree with no limitation on the maximal depth because when there is no limit of depth, the output probability will be hard to varied, the final output is more like a label rather than a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) The prediction of a hard voting is class A, because in hard voting, the result follows with the majority. Here, six votes for A and four votes for not A, $4>6$, so the result is **A**.<br>\n",
    "&emsp;&ensp;The prediction of a soft voting is not class A, because the result is based on average probability. Here average is $\\frac{0.1+0.15+0.2+0.2+0.55+0.6+0.6+0.65+0.7+0.75}{10}=0.45<0.5$, so the result is **not A**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) k-means clustering \n",
    "### (5 points total)\n",
    "\n",
    "\n",
    "**(a)** Use make_blobs from `scikit-learn` to create two datasets  with 2 and 5 cluster centers, each containing 5000 samples with 2 features (0.5 points). \n",
    "\n",
    "**(b)** For both datasets run the k-means algorithm with k=5. Plot the results using different colors to indicate the clusters (0.5 points). \n",
    "\n",
    "**(c)**  Use the cluster centers from (b) to compute the sum of square error (i.e. loop over all datapoints) (2 points). \n",
    "\n",
    "**(d)** For both dataset run the k-means algorithm for values of k from 1 to 10 and then plot the \"elbow curve\" using the sum of square error. (1.5 points)\n",
    "\n",
    "**(e)**  For both dataset, discuss if the elbow method works (0.5 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config lnlineBackend.figure_format = 'retina'\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a) create datasets\n",
    "X2, y2=make_blobs(n_samples=5000, n_features=2, centers=2)#2 clusters\n",
    "X5, y5=make_blobs(n_samples=5000, n_features=2, centers=5)#5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b) k-menas\n",
    "kmodel2=KMeans(n_clusters=5)\n",
    "kmodel5=KMeans(n_clusters=5)\n",
    "y_pred2=kmodel2.fit_predict(X2)# prediction for the 2-clusters data\n",
    "center2=kmodel2.cluster_centers_#the center\n",
    "y_pred5=kmodel5.fit_predict(X5)# prediction for the 5-clusters data\n",
    "center5=kmodel5.cluster_centers_#the center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X2[:, 0], X2[:, 1], c =y_pred2)\n",
    "plt.xlabel('feature x0')\n",
    "plt.ylabel('feature x1')\n",
    "plt.title('The result of two clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X5[:, 0], X5[:, 1], c =y_pred5)\n",
    "plt.xlabel('feature x0')\n",
    "plt.ylabel('feature x1')\n",
    "plt.title('The result of five clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_square_error(k,x,ypredict,center):\n",
    "    #k:number of clusters\n",
    "    #x:the features\n",
    "    #ypredict: the predict \n",
    "    #center: cluster center\n",
    "    sum=0\n",
    "    for i in range(k):\n",
    "        dis=np.sum((x[ypredict==i]-center[i])**2)# the distance between each point to the sample\n",
    "        sum+=dis\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) mean square error\n",
    "SSE2 = sum_square_error(5,X2,y_pred2,center2)\n",
    "SSE5 = sum_square_error(5,X5,y_pred5,center5)\n",
    "print('The mean square error of two clusters is '+str(SSE2))\n",
    "print('The mean square error of five clusters is '+str(SSE5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d) elbow curve\n",
    "# calculate the mean square error for each k value\n",
    "SSE2=[]\n",
    "SSE5=[]\n",
    "K=[i for i in range(1,11)]\n",
    "for k in K:\n",
    "    kmodel2=KMeans(n_clusters=k)#set up model\n",
    "    kmodel5=KMeans(n_clusters=k)\n",
    "    y_pred2=kmodel2.fit_predict(X2)#predict\n",
    "    center2=kmodel2.cluster_centers_#cluster center\n",
    "    y_pred5=kmodel5.fit_predict(X5)\n",
    "    center5=kmodel5.cluster_centers_\n",
    "    SSE2.append(sum_square_error(k,X2,y_pred2,center2))# calculate the sum square error\n",
    "    SSE5.append(sum_square_error(k,X5,y_pred5,center5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, SSE2)\n",
    "plt.locator_params('x',nbins=10)# show ten numbers in x axis\n",
    "plt.xlabel('the value of k')\n",
    "plt.ylabel('mean square error')\n",
    "plt.title('The elbow curve of two clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, SSE5)\n",
    "plt.locator_params('x',nbins=10)# show ten numbers in x axis\n",
    "plt.xlabel('the value of k')\n",
    "plt.ylabel('mean square error')\n",
    "plt.title('The elbow curve of five clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) For the two clusters case, the elbow method **works**, there is a clear elbow at k=2.<br>\n",
    "&emsp;&ensp;&ensp;For the five clusters case, the elbow method seems **do not work**, we can see there are several elbows here, so it is hard to decide which is the real elbow.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Bias-variance tradeoff for the kNN classifier\n",
    "### (3.5 points total)\n",
    "\n",
    "\n",
    "**(a)** use make moon to create a dataset of 1000 random samples with noise=0.35. Scatterplot the dataset. (0.5 points)\n",
    "\n",
    "**(b)** Select 400 of the 1000 data points at random. Use this dataset to train three k-Nearest Neighbor classifiers with $k = \\{1, 20, 140\\}$ (1 point).\n",
    "\n",
    "**(c)** Create three plots showing the three decision boundaries together with the training data (0.5 points). \n",
    "\n",
    "**(d)** Split the dataset from (a) in two equal sized test and training datasets. Train a kNN classifier on your training set for k=1,2,...140. Apply each of these trained classifiers to both your training dataset and your test dataset and plot the classification error (fraction of mislabeled datapoints) using a logarithmic x-axis (1.5 points). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a) create dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.35)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.xlabel('feature x0')\n",
    "plt.ylabel('feature x1')\n",
    "plt.title('the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) train KNN\n",
    "m=np.random.randint(0,1000,400)#choose 400 indexs out of 1000 randomly\n",
    "xtrain=X[m]\n",
    "ytrain=y[m]\n",
    "# create the KNN for each k and see their performance\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn1.fit(xtrain,ytrain)\n",
    "print('The training score of k-Nearest Neighbor classifiers with  ùëò=1:\\n',knn1.score(xtrain,ytrain))\n",
    "knn20 = KNeighborsClassifier(n_neighbors=20)\n",
    "knn20.fit(xtrain,ytrain)\n",
    "print('The training score of k-Nearest Neighbor classifiers with  ùëò=20:\\n',knn20.score(xtrain,ytrain))\n",
    "knn140 = KNeighborsClassifier(n_neighbors=140)\n",
    "knn140.fit(xtrain,ytrain)\n",
    "print('The training score of k-Nearest Neighbor classifiers with  ùëò=140:\\n',knn140.score(xtrain,ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c) show the decision boundaries\n",
    "cm_tree = ListedColormap(['#e58139','#399de5'])\n",
    "h = .01  # step size\n",
    "# determine boundaries\n",
    "x1_min, x1_max = xtrain[:, 0].min() - .5, xtrain[:, 0].max() + .5\n",
    "x2_min, x2_max = xtrain[:, 1].min() - .5, xtrain[:, 1].max() + .5\n",
    "# assign predictions to each mesh point\n",
    "xx1, yy1 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "Z1 = knn1.predict(np.c_[xx1.ravel(), yy1.ravel()])\n",
    "Z1 = Z1.reshape(xx1.shape)\n",
    "# plot training data\n",
    "plt.scatter(xtrain[:, 0], xtrain[:, 1], c=ytrain,  cmap=cm_tree, edgecolors='k')\n",
    "# plot decision boundary\n",
    "plt.contourf(xx1, yy1, Z1, cmap=cm_tree, alpha=.1)\n",
    "plt.xlabel('feature $x_0$', size=16)\n",
    "plt.ylabel('feature $x_1$', size=16)\n",
    "plt.title('The picture of decision boundary and training data with k=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign predictions to each mesh point\n",
    "xx20, yy20 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "Z20 = knn20.predict(np.c_[xx20.ravel(), yy20.ravel()])\n",
    "Z20 = Z20.reshape(xx20.shape)\n",
    "# plot training data\n",
    "plt.scatter(xtrain[:, 0], xtrain[:, 1], c=ytrain,  cmap=cm_tree, edgecolors='k')\n",
    "# plot decision boundary\n",
    "plt.contourf(xx20, yy20, Z20, cmap=cm_tree, alpha=.1)\n",
    "plt.xlabel('feature $x_0$', size=16)\n",
    "plt.ylabel('feature $x_1$', size=16)\n",
    "plt.title('The picture of decision boundary and training data with k=20')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign predictions to each mesh point\n",
    "xx140, yy140 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "Z140 = knn140.predict(np.c_[xx140.ravel(), yy140.ravel()])\n",
    "Z140 = Z140.reshape(xx140.shape)\n",
    "# plot training data\n",
    "plt.scatter(xtrain[:, 0], xtrain[:, 1], c=ytrain,  cmap=cm_tree, edgecolors='k')\n",
    "# plot decision boundary\n",
    "plt.contourf(xx140, yy140, Z140, cmap=cm_tree, alpha=.1)\n",
    "\n",
    "plt.xlabel('feature $x_0$', size=16)\n",
    "plt.ylabel('feature $x_1$', size=16)\n",
    "plt.title('The picture of decision boundary and training data with k=140')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d) plot classification error\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)# split the data set\n",
    "K=[i for i in range(1,141)]\n",
    "totalfraction=[]\n",
    "train=[]\n",
    "test=[]\n",
    "for k in K:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred = knn.predict(X)\n",
    "    totalfraction.append(1-accuracy_score(y, y_pred))# fraction of error classification for the whole data \n",
    "    train.append(1-accuracy_score(y_train,knn.predict(X_train)))\n",
    "    test.append(1-accuracy_score(y_test,knn.predict(X_test)))\n",
    "plt.plot([math.log(i) for i in K],totalfraction,label='the whole data set')\n",
    "plt.plot([math.log(i) for i in K],train,label='training data')\n",
    "plt.plot([math.log(i) for i in K],test,label='test data')\n",
    "plt.xlabel('the value of k (lograthmmic)')\n",
    "plt.ylabel('the classification error')\n",
    "plt.title('The classification error vs. k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
