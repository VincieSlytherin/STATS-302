{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Ran Ju*\n",
    "Netid: rj133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Mathematical description of NN\n",
    "### (2.5 points)\n",
    "\n",
    "Let's assume you have a deep neural network with 10 input neurons, one hidden layer with 50 neurons, and one output layer with 3 neurons. All neurons use the hyperbolic tangent as activation. \n",
    "\n",
    "**(a)** What are the dimensions of a pair of feature and target variables $\\bf x_i$  and $\\bf y_i$? (*0.5 points*)\n",
    "\n",
    "**(b)** What are the dimensions of the first weight matrix $\\bf w_1$ and the corresponding bias vector $\\bf b_1$?  (*0.5 points*)  \n",
    "\n",
    "**(c)** What are the dimensions of the weight matrix $\\bf w_2$ and the bias vector $\\bf b_2$ of the output layer?   (*0.5 points*)\n",
    "\n",
    "**(d)** Write down the equation to compute $\\bf y_i$. (*0.5 points*)\n",
    "\n",
    "**(e)** How many trainable parameters does this network have? (*0.5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** The dimensions of $(x_i,y_i)$ are $(10\\times1,3\\times1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** The dimensions of $w_1$ is $50\\times10$ and the dimension of $b_1$ is $50\\times1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** The dimensions of $w_2$ is $3\\times50$ and the dimension of $b_2$ is $3\\times1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** $z_1=w_1x_i+b_1, a_1=tanh(z_1), z_2=w_2a_1+b_2, y_i=tanh(z_2)$ where $tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e)** There are totally 703 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$50\\times(10+1)+3\\times(50+1)=703$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "model_simple = Sequential()\n",
    "model_simple.add(Dense(50, activation='relu', input_shape=(10,)))# the hidden has 50 neurons and the input has 10 features\n",
    "model_simple.add(Dense(3, activation='softmax'))#the output layer has 3 neurons\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Classification with a deep neural network \n",
    "### (5 points )\n",
    "\n",
    "**(a)** Create 1000 training and 400 test data points with the make_moons function from Scikit Learn. Set the noise level to 0.2. (*0.5 points*)\n",
    "\n",
    "**(b)**  Design a neural network using Keras. The first hidden layer has 100 neurons with rectified linear units as activation. The second hidden layer has 25 neurons and also rectified linear units as activation. The output layer uses \n",
    "sigmoid activation. The loss function is binary crossentropy, the gradient descent method is Adam and the metric used for evaluation is accuracy. (*1 point*)\n",
    "\n",
    "**(c)** Train the network with a batch size of 64 and early stopping if the validation loss does not change over 4 epochs. Report the test accuracy. (*1 point*)\n",
    "\n",
    "**(d)** Plot the test data points together with a mesh indicating the prediction of the neural network. You can reuse the code from notebook 04_07.  (*1 point*)\n",
    "\n",
    "**(e)** Make two figures showing the evolution of loss and accuracy as a function of number of epochs. In both figures report training and test results.(*0.5 point*)\n",
    "\n",
    "**(f)** Describe *in words* how the results change (if at all) when you change the batch size to 32 and 128. (*0.5 points*)\n",
    "\n",
    "**(g)** Describe *in words* how the results change (if at all) when you change the the activation of the two hidden layers to sigmoid. (*0.5 points*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a) create data points\n",
    "from sklearn.datasets import make_moons\n",
    "trainx,trainy=make_moons(n_samples=1000,noise=0.2) #training data\n",
    "testx,testy=make_moons(n_samples=400,noise=0.2) #test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b) create neural networks\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2,)))# input shape is (2,) \n",
    "                                                    #because the shape of trainx is (1000,2)\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c) train the network\n",
    "batch_size=64\n",
    "epochs=100# set it to 100 because I tried several numbers and I find 100 is the safest value here\n",
    "# the model should stop training when it won't improve anymore\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor=EarlyStopping(monitor='val_loss', patience=4)# not change over 4 epochs\n",
    "history_simple=model.fit(trainx,trainy,batch_size=batch_size,epochs=epochs,verbose=1,callbacks=[early_stopping_monitor],              \n",
    "                    validation_data=(testx,testy))\n",
    "\n",
    "# evaluate model performance\n",
    "score = model.evaluate(testx,testy,verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(d) plot the test data\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "%config lnlineBackend.figure_format = 'retina'\n",
    "# 1) determine boundaries\n",
    "# compute the decision boundary on a grid, for later visualization\n",
    "\n",
    "X=testx\n",
    "x1_min,x1_max=X[:,0].min()-.5,X[:,0].max()+.5\n",
    "x2_min,x2_max=X[:,1].min()-.5,X[:,1].max()+.5\n",
    "\n",
    "# 2) create a mesh of size [x1_min, x1_max] x [x2_min, x2_max].\n",
    "h=.02  # step size in the mesh\n",
    "xx,yy=np.meshgrid(np.arange(x1_min,x1_max,h), np.arange(x2_min,x2_max,h))\n",
    "\n",
    "# 3) assign logistic regression prediction to each mesh point\n",
    "Z=model.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "\n",
    "# prepare colormaps\n",
    "cm=plt.cm.PiYG\n",
    "cm_bright=ListedColormap(['#b30065','#178000'])\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# plot the prediction result using the mesh\n",
    "Z=Z.reshape(xx.shape)\n",
    "plt.contourf(xx,yy,Z,cmap=cm, alpha=.2)\n",
    "\n",
    "# and test points\n",
    "plt.scatter(testx[:,0],testx[:,1],c=testy,cmap=cm_bright,edgecolors='k',alpha=0.6)\n",
    "\n",
    "plt.xlabel('feature $x_1$',size=16)\n",
    "plt.ylabel('feature $x_2$',size=16)\n",
    "plt.title('test data',size=20)\n",
    "\n",
    "plt.xlim(xx.min(),xx.max())\n",
    "plt.ylim(yy.min(),yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(e)show the learning process\n",
    "model=Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2,)))# input shape is (2,) \n",
    "                                                    #because the shape of trainx is (1000,2)\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "batch_size=64\n",
    "epochs=100\n",
    "\n",
    "history=model.fit(trainx, trainy,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(testx, testy))\n",
    "\n",
    "# evaluate model performance\n",
    "score=model.evaluate(testx,testy,verbose=0)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=history.history['accuracy']\n",
    "val_accuracy=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(len(accuracy))\n",
    "#accuracy\n",
    "plt.plot(epochs,accuracy,'bo',label='Training')#training data\n",
    "plt.plot(epochs,val_accuracy,'r',label='Test')#test data\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Accuracy',size=14)\n",
    "plt.title('Evolution of accuracy',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))#because there are 100 epochs\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#loss\n",
    "plt.figure()\n",
    "plt.plot(epochs,loss,'bo',label='Training')#training\n",
    "plt.plot(epochs,val_loss,'r',label='Test')#test\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Loss',size=14)\n",
    "plt.title('Evolution of loss function',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(f) batch size change\n",
    "#First draw the figures\n",
    "model=Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2,)))# input shape is (2,) \n",
    "                                                    #because the shape of trainx is (1000,2)\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "batch_size=32\n",
    "epochs=100\n",
    "history=model.fit(trainx,trainy,batch_size=batch_size,epochs=epochs,verbose=1,  validation_data=(testx, testy))\n",
    "# evaluate model performance\n",
    "score=model.evaluate(testx,testy,verbose=0)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=history.history['accuracy']\n",
    "val_accuracy=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(len(accuracy))\n",
    "plt.plot(epochs,accuracy,'bo',label='Training')\n",
    "plt.plot(epochs,val_accuracy,'r',label='Test')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Accuracy',size=14)\n",
    "plt.title('Evolution of accuracy',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(epochs,loss,'bo',label='Training')\n",
    "plt.plot(epochs,val_loss,'r',label='Test')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Loss',size=14)\n",
    "plt.title('evolution of loss function',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(100,activation='relu',input_shape=(2,)))# input shape is (2,) \n",
    "                                                    #because the shape of trainx is (1000,2)\n",
    "model.add(Dense(25,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "batch_size=128\n",
    "epochs=100\n",
    "\n",
    "history=model.fit(trainx,trainy,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(testx, testy))\n",
    "\n",
    "# evaluate model performance\n",
    "score=model.evaluate(testx,testy,verbose=0)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=history.history['accuracy']\n",
    "val_accuracy=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(len(accuracy))\n",
    "plt.plot(epochs,accuracy,'bo',label='Training')\n",
    "plt.plot(epochs,val_accuracy,'r',label='Test')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Accuracy',size=14)\n",
    "plt.title('Evolution of accuracy',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(epochs,loss,'bo',label='Training')\n",
    "plt.plot(epochs,val_loss,'r',label='Test')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Loss',size=14)\n",
    "plt.title('evolution of loss function',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: everytime we need to rebuild the model to avoid the model is based on the training results before.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the accuracy and loss are almost the same but the figure with the batch size 128 seems to be more flat and in the case of batch size as 32, 64 and 128, when the batch size gets larger, the learning process curve seems more smooth. The mesh figures seem almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Internet, I find that batch size will also have effect in the time. When the batch size is too small it will take a lot of time, but here it seems there is no much difference. Meanwhile, the gradient oscillation will be  serious, which is not conducive to convergence. If the batch size is too large, the gradient direction of different batches does not change at all, so it is easy to fall into local minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (g) activation change\n",
    "modelsig=Sequential()\n",
    "modelsig.add(Dense(100,activation='sigmoid',input_shape=(2,)))\n",
    "modelsig.add(Dense(25,activation='sigmoid'))\n",
    "modelsig.add(Dense(1,activation='sigmoid'))\n",
    "modelsig.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "batch_size=128# compare it with above 128 batch size 100 epochs with relu activation\n",
    "epochs=100\n",
    "history=modelsig.fit(trainx,trainy,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(testx,testy))\n",
    "# evaluate modelsig performance\n",
    "score=modelsig.evaluate(testx,testy,verbose=0)\n",
    "print('Test loss:',score[0])\n",
    "print('Test accuracy:',score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=history.history['accuracy']\n",
    "val_accuracy=history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "epochs=range(len(accuracy))\n",
    "plt.plot(epochs,accuracy,'bo',label='Training')\n",
    "plt.plot(epochs,val_accuracy,'r',label='Test')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Accuracy',size=14)\n",
    "plt.title('Evolution of accuracy',size=16)\n",
    "plt.xticks(np.arange(0, 100, step=5))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(epochs,loss, 'bo',label='Training')\n",
    "plt.plot(epochs,val_loss,'r',label='Test')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Loss',size=14)\n",
    "plt.title('Evolution of loss function',size=16)\n",
    "plt.xticks(np.arange(0,100,step=5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=testx\n",
    "x1_min,x1_max=X[:,0].min()-.5,X[:,0].max()+.5\n",
    "x2_min,x2_max=X[:,1].min()-.5,X[:,1].max()+.5\n",
    "\n",
    "# 2) create a mesh of size [x1_min, x1_max] x [x2_min, x2_max].\n",
    "h=.02  # step size in the mesh\n",
    "xx,yy=np.meshgrid(np.arange(x1_min,x1_max,h), np.arange(x2_min,x2_max,h))\n",
    "\n",
    "# 3) assign logistic regression prediction to each mesh point\n",
    "Z=modelsig.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "\n",
    "# prepare colormaps\n",
    "cm=plt.cm.PiYG\n",
    "cm_bright=ListedColormap(['#b30065','#178000'])\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "# plot the prediction result using the mesh\n",
    "Z=Z.reshape(xx.shape)\n",
    "plt.contourf(xx,yy,Z,cmap=cm, alpha=.2)\n",
    "\n",
    "# and test points\n",
    "plt.scatter(testx[:,0],testx[:,1],c=testy,cmap=cm_bright,edgecolors='k',alpha=0.6)\n",
    "\n",
    "plt.xlabel('feature $x_1$',size=16)\n",
    "plt.ylabel('feature $x_2$',size=16)\n",
    "plt.title('test data',size=20)\n",
    "\n",
    "plt.xlim(xx.min(),xx.max())\n",
    "plt.ylim(yy.min(),yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When other things remain the same, the accuracy will decrease and the loss will get larger (I tried it with batch size 32, 64 and 128, for convenience here I only show the result of 128). The trend of loss and accuracy of the test data with epoch seems to be more alikely to the training data. And the mesh with sigmoid function becomes straight lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is becuase the sigmoid function compress the data into the interval $(0,1)$ and the ReLu function drop some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Demonstration of the universal approximation theorem\n",
    "### (2.5 points total)\n",
    "\n",
    "\n",
    "**(a)**  Write a function which computes $f(x)=0.2 + 0.4 \\;x^2 + 0.3 \\;x\\; sin(9x)$. Create a vector x_train which contains 10000 evenly spaced points between 0 and 1. Compute the vector y_train = f(x_train). Plot y_train versus x_train. This is the function we want to approximate with a neural network containing one hidden layer. (*0.5 points*)\n",
    "\n",
    "**(b)**  Create a neural network with one input neuron, a hidden layer with 50 neurons and sigmoid activation and one output neuron with linear activation. Choose Mean Squared Error as loss function and Adam(lr=0.005) as gradient descent method. Train the model with a batch size of 2000 for 4000 epochs. (We do not need test data in this demonstration.)\n",
    "\n",
    "After training the network make a prediction using x_train and plot this prediction together with y_train (i.e. the function the network tries to approximate). (*1.5 points*)\n",
    "\n",
    "**(c)**  Plot the evolution of the loss function with a logarithmic y-axis. Describe *in words* how this curve changes\n",
    "when you change the hyperparameter learning rate for Adam to lr=0.002. \n",
    "(*0.5 points*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(a) write the function and build training data\n",
    "import math\n",
    "def f(x):\n",
    "    fx=0.2+.4*x*x+0.3*x*math.sin(9*x)\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config lnlineBackend.figure_format = 'retina'\n",
    "x_train=np.linspace(0,1,num=10000)#crate training data\n",
    "y_train=[]\n",
    "for i in x_train:\n",
    "    y_train.append(f(i))\n",
    "\n",
    "plt.plot(x_train,y_train)\n",
    "plt.xlabel('evenly spaced training points x')\n",
    "plt.ylabel('the computed training targets y')\n",
    "plt.title('y_train versus x_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b) build neural netwoek\n",
    "model=Sequential()\n",
    "model.add(Dense(50,activation='sigmoid',input_shape=(1,)))\n",
    "model.add(Dense(1,activation='linear'))\n",
    "model.compile(loss='mean_squared_error',optimizer=Adam(lr=0.005))\n",
    "# train the model\n",
    "batch_size=2000\n",
    "epochs=4000\n",
    "\n",
    "# the model should stop training when it won't improve anymore\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor=EarlyStopping(monitor='val_loss', patience=4)\n",
    "\n",
    "history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model.predict(x_train)\n",
    "plt.plot(x_train,y_train,label='real y',alpha=0.8)\n",
    "plt.plot(x_train,y_predict,label='predicted y',alpha=0.8)\n",
    "plt.xlabel('evenly spaced training points x')\n",
    "plt.ylabel('the targets y')\n",
    "plt.title('y_train and y_predict versus x_train')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(c) the evolution \n",
    "loss = history.history['loss']\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, np.log(loss),label='Training')\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Loss (logarithmic)',size=14)\n",
    "plt.title('evolution of the loss function with lr=0.005',size=16)\n",
    "plt.xticks(np.arange(0,4000,step=1000))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelc=Sequential()\n",
    "modelc.add(Dense(50,activation='sigmoid',input_shape=(1,)))\n",
    "modelc.add(Dense(1,activation='linear'))\n",
    "modelc.compile(loss='mean_squared_error',optimizer=Adam(lr=0.002))# change the lr to 0.002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "batch_size=2000\n",
    "epochs=4000\n",
    "historyc=modelc.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossc=historyc.history['loss']\n",
    "epochs=range(len(loss))\n",
    "plt.plot(epochs, np.log(lossc),label='Training lr=0.002',alpha=0.5)\n",
    "plt.plot(epochs, np.log(loss),label='Training lr=0.005',alpha=0.5)\n",
    "plt.xlabel('Epoch',size=14)\n",
    "plt.ylabel('Loss (logarithmic)', size=14)\n",
    "plt.title('evolution of the loss function with different lr')\n",
    "plt.xticks(np.arange(0,4000,step=1000))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that when lr=0.002, the loss seems larger and the curve are centered when the number of epoches gets larger and the loss value seems decrease faster when lr=0.005. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it is because the learing rate 0.005 is too big so it causes the float when theremare more epochs and the learning rate 0.002 is too small so the loss decreases slowly and it has no chance to float."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
